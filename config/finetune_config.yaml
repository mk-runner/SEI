# =========data config===========#
"mimic_cxr_image_dir": "/home/miao/data/dataset/MIMIC-CXR/files/"
# "mimic_cxr_ann_path": "knowledge_encoder/mimic_cxr_annotation_sen_best_reports_keywords_20.json"
"iu_xray_image_dir": "/home/miao/data/dataset/iu_xray/images/"
# "iu_xray_ann_path": "knowledge_encoder/iu_xray_annotation_sen_best_reports_keywords_20.json"
#"max_seq_len": 100  # in train, test, and val data set, the max sequence length is 384 (train: 336, val: 229, test: 384)
"num_workers": 10
#"batch_size": 16


# =========tokenizer config=======#
"tokenizer_dir": "../config/tokenizer/"
#"tokenizer_model": 'wordlevel'    # choices = ['wordlevel', 'wordpiece']
#"tokenizer_type": 'uncased'       # choices = ["case", 'uncased']


# =========specific knowledge config=======#
"sk_analysis_dir": "results/visual_specific_knowledge/current_model/"
#"sk_type": 'report'  # choices = ['report', 'keywords']
#"sk_topk": 5

# ========= metrics checkpoint config =====#
#"bertscore_checkpoint": "/home/20031211471/Data/checkpoints/roberta-large"
#"bertscore_checkpoint": "/home/miao/data/dataset/checkpoints/distilbert-base-uncased"
#"chexbert_checkpoint": "/home/miao/data/dataset/checkpoints/chexbert.pth"
#"chexbert_model_checkpoint": "/home/miao/data/dataset/checkpoints/bert-base-uncased"
#"chexbert_tokenizer_checkpoint": "/home/miao/data/dataset/checkpoints/bert-base-uncased"
#"radgraph_checkpoint": "/home/miao/data/dataset/checkpoints/radgraph"
#"radgraph_reward_level": 'partial'    # ["simple", "partial", "complete", "all"]


# =========model config===========#
# ====visual encoder config====#
#"visual_encoder": 'resnet101'     # choices = [resnet101, ViT-B-32]
# "resnet_checkpoint": '/home/miao/data/dataset/checkpoints/resnet101-5d3b4d8f.pth'
# "vit_checkpoint": ''

# ====language model config===#
# "text_checkpoint": '/home/miao/data/dataset/checkpoints/scibert_scivocab_uncased'
#"text_decoder": 'r2gen'           # choices = [CMN, r2gen, bert]

# text encoder config
"encoder_hidden_size": 768
"encoder_num_hidden_layers": 6

# text encoder and image encoder fusion module config
"fusion_num_heads": 8
#"fusion_checkpoint": "bert-base-uncased"


# text decoder (bert) config
"decoder_hidden_size": 2048
"decoder_num_attention_heads": 8
"decoder_num_hidden_layers": 3

# text decoder (r2gen+cmn) config
"num_heads": 8
"num_layers": 3
"d_model": 512    # the dimension of Transformer.
"d_ff": 512       # the dimension of FFN.
"d_vf": 2048      # the dimension of the patch feature
"dropout": 0.0
"drop_prob_lm": 0.5
"logit_layers": 1
"use_bn": 0
# Relational Memory (r2gen) config
"rm_num_slots": 3
"rm_num_heads": 8
"rm_d_model": 512
# report generation (r2gen) config
"sample_method": "beam_search"
"length_penalty": ""
"diversity_lambda": 0.5
"suppress_UNK": 0
"temperature": 1.0     # for report generation
"group_size": 1
"sample_n": 1
"output_logsoftmax": 1
"decoding_constraint": 0
"block_trigrams": 1

# memory network (CMN) config
"topk": 32        # the number of k for memory network
"cmm_size": 2048  # the number of cmm size.
"cmm_dim": 512    # the dimension of cmn

# text decoder generate config
"beam_size": 3

# visual/language local embedding and global embedding
"output_dim": 2048
"proj_num_heads": 8

# ===============loss config================#
"instance_temp": 0.5   # the temperature parameter of instance-level contrastive learning
"region_temp": 0.5     # the temperature parameter of region-level contrastive learning


#===========trainer config===========#
#"epochs": 50
"seed": 9233
"result_dir": "results"
#"record_dir": "records/finetune"
"save_period": 1
"early_stop": 10
"monitor_metric_curves": true
"monitor_report": true
"monitor_image": true
# optim config
#"optim": "AdamW"   # choices = ['AdamW', 'RAdam']
#"lr": 5.0e-5   # note cannot 5e-4
"weight_decay": 1.0e-4
"amsgrad": true
#"lr_scheduler": "ReduceLROnPlateau"  # choice = ['StepLR', 'ReduceLROnPlateau']
"step_size": 10
"gamma": 0.5
# other config
"n_gpu": 1
# finetune config
"ft_lr_monitor_metric": "F1-Radgraph-partial"
"ft_monitor_mode": "max"
"ft_monitor_metric": "F1-Radgraph-partial"
#"freeze_image_encoder": true
#"freeze_text_encoder": false
# pretrain config
"pt_monitor_mode": "min"
"pt_monitor_metric": "all_loss"
"pt_lr_monitor_metric": "all_loss"
