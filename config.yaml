# =========data config===========#
"mimic_cxr_image_dir": "/media/miao/data/Dataset/MIMIC-CXR/mimic-cxr-images/files"
"mimic_cxr_ann_path": "knowledge_encoder/mimic_cxr_annotation_sen.json"
"iu_xray_image_dir": "/media/miao/data/Dataset/iu_xray/images"
"iu_xray_ann_path": "/media/miao/data/Dataset/iu_xray/annotation.json"
"mimic_threshold": 10
"iu_threshold": 3
"max_seq_length": 100
"num_workers": 8
"batch_size": 32


# =========tokenizer config=======#
"tokenizer_dir": "config/tokenizer/"
"tokenizer_model": 'wordlevel'    # choices = ['wordlevel', 'wordpiece']
"tokenizer_type": 'uncased'       # choices = ["case", 'uncased']


# =========specific knowledge config=======#
"sk_analysis_dir": "results/visual_specific_knowledge/current_model/"
"sk_topk": 20

# ========= metrics checkpoint config =====#
"bertscore_checkpoint": "/media/miao/data/Dataset/checkpoint/roberta-large"
"chexbert_checkpoint": "/media/miao/data/Dataset/checkpoint/chexbert.pth"
"chexbert_model_checkpoint": "/media/miao/data/Dataset/checkpoint/bert-base-uncased"
"chexbert_tokenizer_checkpoint": "/media/miao/data/Dataset/checkpoint/bert-base-uncased"
"radgraph_checkpoint": "/media/miao/data/Dataset/checkpoint/radgraph"
"radgraph_reward_level": 'partial'    # ["simple", "partial", "complete", "all"]


# =========model config===========#
# ====visual encoder config====#
"visual_encoder": 'resnet101'     # choices = [resnet101, ViT-B-32]
"resnet_checkpoint": '/media/miao/data/Dataset/checkpoint/resnet101-5d3b4d8f.pth'
"vit_checkpoint": '/media/miao/data/Dataset/checkpoint/ViT-B-32.pt'

# ====language model config===#
"text_checkpoint": '/media/miao/data/Dataset/checkpoint/scibert_scivocab_uncased'
"text_decoder": 'bert'           # choices = [CMN, r2gen, bert]

# text encoder config
"encoder_hidden_size": 768
"encoder_num_hidden_layers": 6

# text decoder (bert) config
"decoder_hidden_size": 2048
"decoder_num_hidden_layers": 3

# text decoder (r2gen+cmn) config
"num_heads": 8
"num_layers": 3
"d_model": 512    # the dimension of Transformer.
"d_ff": 512       # the dimension of FFN.
"d_vf": 2048      # the dimension of the patch feature
"dropout": 0.0
"drop_prob_lm": 0.5
"logit_layers": 1
"use_bn": 0
# Relational Memory (r2gen) config
"rm_num_slots": 3
"rm_num_heads": 8
"rm_d_model": 512
# report generation (r2gen) config
"sample_method": "beam_search"
"length_penalty": ""
"diversity_lambda": 0.5
"suppress_UNK": 0
"temperature": 1.0     # for report generation
"group_size": 1
"sample_n": 1
"output_logsoftmax": 1
"decoding_constraint": 0
"block_trigrams": 1

# memory network (CMN) config
"topk": 32        # the number of k for memory network
"cmm_size": 2048  # the number of cmm size.
"cmm_dim": 512    # the dimension of cmn

# text decoder generate config
"beam_size": 3

# visual/language local embedding and global embedding
"output_dim": 2048
"proj_num_heads": 8

# ===============loss config================#
"instance_temp": 0.5   # the temperature parameter of instance-level contrastive learning
"region_temp": 0.5     # the temperature parameter of region-level contrastive learning


#===========trainer config===========#
"epochs": 100
"seed": 9233
"result_dir": "results"
#"record_dir": "records/finetune"
"save_period": 1
"early_stop": 10
"monitor_metric_curves": true
"monitor_report": true
"monitor_image": true
# optim config
"optim": "AdamW"
"lr": 5.0e-5   # note cannot 5e-4
"weight_decay": 1.0e-4
"amsgrad": true
#"lr_scheduler": "warmup"
"step_size": 30
"gamma": 0.1
# other config
"n_gpu": 1
# finetune config
"ft_monitor_mode": "max"
"ft_monitor_metric": "BLEU_4"
"freeze_encoders": true
# pretrain config
"pt_monitor_mode": "min"
"pt_monitor_metric": "all_loss"